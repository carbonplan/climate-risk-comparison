{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77b7cb76-821a-4dd9-baf3-c176ab244adb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import numpy as np\n",
    "from scipy.stats import kendalltau\n",
    "import scipy as sp\n",
    "import sklearn as sk\n",
    "import json\n",
    "import copy \n",
    "pd.set_option('display.max_rows', 30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cfbd4e2-5b96-4ce9-b984-53f55caee6fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "jup_conversion = {'Lowest': 1,\n",
    "                  'Low': 2,\n",
    "                 'Medium': 3,\n",
    "                 'High': 4,\n",
    "                 'Highest': 5}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7ac2ba5-b081-4fb8-aa95-e485f34bb25a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_jupiter_file(file_path, variable):\n",
    "    jupiter = pd.read_csv(file_path)\n",
    "    full_df_jupiter = pd.DataFrame()\n",
    "    for scenario in ['SSP1-2.6 (1.8C)', 'SSP2-4.5 (2.7C)','SSP5-8.5 (4.4C)']:\n",
    "        scenario_subset = jupiter[jupiter['scenario']==scenario]\n",
    "        # use the pivot_table utility in pandas to convert the locations into the index values\n",
    "        # and the time and scenario as the columns\n",
    "        scenario_subset = scenario_subset.pivot_table(index='locationId', \n",
    "                                                      columns='year', \n",
    "                                                      values=variable, \n",
    "                                                      aggfunc='first', \n",
    "                                                      dropna=False)\n",
    "        scenario_subset.columns = [f'{scenario} - {year}' for year in np.arange(2020, 2101, 5)]\n",
    "        full_df_jupiter = pd.concat([full_df_jupiter, scenario_subset], axis=1)\n",
    "    full_df_jupiter['Historical baseline - 1995'] = jupiter[jupiter['scenario']=='Historical Baseline'][variable].values\n",
    "    full_df_jupiter.index = full_df_jupiter.index-1\n",
    "    return full_df_jupiter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c5f7d30-b734-4de1-bea5-39b063a5381e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_jupiter_ca(risks):\n",
    "    _data = load_jupiter_file('s3://carbonplan-climate-impacts/climate-risk-comparison/companies/data/CA-Fire-for-CarbonPlan-Input_CA-Fire-for-CarbonPlan_Enhanced_20240502.csv',\n",
    "                            'FR_annualFireProbability_tier')\n",
    "    for col in _data.columns:\n",
    "        _data[col] = _data[col].map(jup_conversion)\n",
    "    risks['ca']['jupiter'] = locations['ca'].join(_data)[['geometry', \n",
    "                                                           'Historical baseline - 1995', \n",
    "                                                           'SSP5-8.5 (4.4C) - 2100']]\n",
    "    return risks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3dc3439-e0cb-4d22-b7a8-0992a1b25d38",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_jupiter_ny(risks, metric='score'):\n",
    "    risks['nystate']['jupiter'] = {}\n",
    "    risks['nyc']['jupiter'] = {}\n",
    "    variable_shortname_conversions = {'PF': 'Pluvial',\n",
    "                                 'FF': 'Fluvial',\n",
    "                                 'CF': 'Coastal'}\n",
    "    for variable in ['PF', 'CF', 'FF']:\n",
    "        # use the 100-year return period for flooding as it is \n",
    "        _data = load_jupiter_file('s3://carbonplan-climate-impacts/climate-risk-comparison/companies/data/NY-Flood-for-CarbonPlan-Input_NY-Flood-for-CarbonPlan_Enhanced_20240502.csv',\n",
    "                             f'{variable}_depth100yr_tier') \n",
    "        for col in _data.columns:\n",
    "            _data[col] = _data[col].map(jup_conversion) \n",
    "        _data.replace(-9999, np.nan, inplace=True)\n",
    "        locations['ny'].index = np.arange(0,214)\n",
    "        locations['ny'] = locations['ny']\n",
    "        # split the nyc and nystate locations\n",
    "        risks['nystate']['jupiter'][variable_name_conversion_dict[variable]] = locations['ny'].join(_data).iloc[0:124][['geometry', \n",
    "                                                                                                                        'Historical baseline - 1995', \n",
    "                                                                                                                        'SSP5-8.5 (4.4C) - 2100']]\n",
    "        risks['nyc']['jupiter'][variable_name_conversion_dict[variable]] = locations['ny'].join(_data).iloc[124:][['geometry', \n",
    "                                                                                                                   'Historical baseline - 1995', \n",
    "                                                                                                                   'SSP5-8.5 (4.4C) - 2100']]\n",
    "    return risks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f0e7e10-61ca-4987-8780-70378eb98ba0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_xdi(region, risks, metric='score'):\n",
    "    # region is either 'NY' or 'california'\n",
    "    if region=='NY':\n",
    "        risks['nyc']['xdi'] = {}\n",
    "        risks['nystate']['xdi'] = {}\n",
    "    elif region=='california':\n",
    "        risks['ca']['xdi'] = {}\n",
    "    df = pd.read_excel(f's3://carbonplan-climate-impacts/climate-risk-comparison/companies/data/{region}input_merged.xlsx', \n",
    "                       sheet_name='hazABC.csv')\n",
    "    variables_dict = {'california': ['Forest Fire'],\n",
    "                     'NY': ['Riverine Flooding', 'Coastal Inundation', 'Surface Water Flooding']}\n",
    "    \n",
    "    # According to their documentation, XDI has a hazard rating system with three tiers. We assign them\n",
    "    # to the values 1,3,5 arbitrarily to match the Jupiter five tier scale.\n",
    "    category_conversion_dict = {'A': 1, # 'Low'\n",
    "                       'B': 3, # 'Medium'\n",
    "                       'C': 5} # 'High'\n",
    "    \n",
    "    for variable in variables_dict[region]:\n",
    "        _data = df.pivot_table(index='ID', columns='Year', values=variable, aggfunc='first')\n",
    "        _data.columns = [f'RCP85-{year}' for year in _data.columns]\n",
    "        for col in _data.columns:\n",
    "            _data[col] = _data[col].map(category_conversion_dict)\n",
    "        if region=='NY':\n",
    "            _data.index = _data.index.astype('str')\n",
    "            risks['nystate']['xdi'][variable] = locations['ny'].join(_data).iloc[0:124][['geometry', 'RCP85-1995', 'RCP85-2100']]\n",
    "            risks['nyc']['xdi'][variable] = locations['ny'].join(_data).iloc[124:][['geometry', 'RCP85-1995', 'RCP85-2100']]\n",
    "        elif region=='california':\n",
    "            _data.index -= 1 # index is offset by 1 in the california file\n",
    "            _data.index = _data.index.astype('str')\n",
    "            # use 1995 as the year representative of historical\n",
    "            risks['ca']['xdi'] = locations['ca'].join(_data)[['geometry', 'RCP85-1995', 'RCP85-2100']]\n",
    "    return risks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df18b674-f95f-44a7-9126-0c397b1e932a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_kendalltau(xdi, jupiter, variant):    \n",
    "    tau, p_value = kendalltau(xdi, jupiter, variant=variant)#, nan_policy='omit')\n",
    "    dist = []\n",
    "\n",
    "    for i in range(1000):\n",
    "        indices = sk.utils.resample(np.arange(xdi.shape[0]), replace=True, random_state=i)\n",
    "        dist.append(sp.stats.kendalltau(xdi.values[indices], jupiter.values[indices], variant=variant)[0])\n",
    "    dist = np.asarray(dist)\n",
    "    # Calculate an error bar with one standard deviation\n",
    "    tau_error = ((np.median(dist) - np.percentile(dist,[16,84])[0]) + (np.percentile(dist,[16,84])[1] - np.median(dist))) / 2\n",
    "    \n",
    "    return tau, p_value, tau_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1deae45-d805-4878-8d6d-9c165e3f0d30",
   "metadata": {},
   "outputs": [],
   "source": [
    "def consolidate_risks(risks):\n",
    "    consolidated = {}\n",
    "    jup = risks['ca']['jupiter'].rename({'Historical baseline - 1995': 'Jupiter - Fire - California - Historical',\n",
    "                               'SSP5-8.5 (4.4C) - 2100': 'Jupiter - Fire - California - 2100'}, axis=1)\n",
    "    xdi = risks['ca']['xdi'].rename({'RCP85-1995': 'XDI - Fire - California - Historical',\n",
    "                                         'RCP85-2100': 'XDI - Fire - California - 2100'}, axis=1)\n",
    "    xdi.index = xdi.index.astype('int')\n",
    "    consolidated['ca'] = pd.concat([jup, xdi[['XDI - Fire - California - Historical',\n",
    "                               'XDI - Fire - California - 2100']]], axis=1)\n",
    "\n",
    "    for region in ['nyc', 'nystate']:\n",
    "        first_flag = True\n",
    "        for variable in ['Surface Water Flooding', 'Riverine Flooding', 'Coastal Inundation']:\n",
    "            jup = risks[region]['jupiter'][variable].rename({'Historical baseline - 1995': f'Jupiter - {variable} - {region} - Historical',\n",
    "                                           'SSP5-8.5 (4.4C) - 2100': f'Jupiter - {variable} - {region} - 2100'}, axis=1)\n",
    "            xdi = risks[region]['xdi'][variable].rename({'RCP85-1995': f'XDI - {variable} - {region} - Historical',\n",
    "                                                 'RCP85-2100': f'XDI - {variable} - {region} - 2100'}, axis=1)\n",
    "            xdi.index = xdi.index.astype('int')\n",
    "            if first_flag:\n",
    "                consolidated[region] = pd.concat([jup, xdi[[f'XDI - {variable} - {region} - Historical',\n",
    "                                       f'XDI - {variable} - {region} - 2100']]], axis=1)\n",
    "                first_flag = False\n",
    "            else:\n",
    "                consolidated[region] = pd.concat([consolidated[region],\n",
    "                                                  jup[[f'Jupiter - {variable} - {region} - Historical',\n",
    "                                       f'Jupiter - {variable} - {region} - 2100']], \n",
    "                                        xdi[[f'XDI - {variable} - {region} - Historical',\n",
    "                                       f'XDI - {variable} - {region} - 2100']]], axis=1)\n",
    "    return consolidated"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6a20c58-b4cd-46c0-8d6e-f07343f3fedf",
   "metadata": {},
   "source": [
    "# Load shapefiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e0b1422-f575-4556-b234-ecbe5622506f",
   "metadata": {},
   "outputs": [],
   "source": [
    "states = gpd.read_file('s3://carbonplan-climate-impacts/climate-risk-comparison/location-selection/cb_2018_us_state_5m/cb_2018_us_state_5m.shp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f645b44-a6c9-4887-b2b8-142a3b726bf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "locations = {}\n",
    "for state in ['ca', 'ny']:\n",
    "    locations[state] = gpd.read_file(f's3://carbonplan-climate-impacts/climate-risk-comparison/location-selection/requested_locations_{state}.json')\n",
    "    locations[state]['ID'] = locations[state].index"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0913c3e9-da92-4168-ad21-729fcd09f462",
   "metadata": {},
   "source": [
    "# Load Jupiter data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb7ea608-2813-4540-aa25-0aca738677c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "risks = {'ca': {},\n",
    "        'nystate': {},\n",
    "        'nyc': {}}\n",
    "for region in ['ca', 'ny']:\n",
    "    locations[region].index = locations[region].index.astype('int')\n",
    "risks = load_jupiter_ca(risks)\n",
    "variable_name_conversion_dict = {'PF': 'Surface Water Flooding',\n",
    "                          'FF':'Riverine Flooding',\n",
    "                          'CF': 'Coastal Inundation'}\n",
    "risks = load_jupiter_ny(risks)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b99d5746-fe62-4342-972a-a28bbd7faf45",
   "metadata": {},
   "source": [
    "# Load XDI data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bea7a50-7033-4328-bfae-eb548675a82d",
   "metadata": {},
   "outputs": [],
   "source": [
    "for region in ['ca', 'ny']:\n",
    "    locations[region].index = locations[region].index.astype('str')\n",
    "\n",
    "risks = load_xdi('california', risks)\n",
    "risks = load_xdi('NY', risks)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a87f8a9-e80a-4e2d-9635-48ba297a8bc8",
   "metadata": {},
   "source": [
    "# Write out data to json for web article"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7774ebc9-fbaf-4fd3-8326-0a18bb764d59",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_for_article = copy.deepcopy(risks)\n",
    "for company in ['xdi', 'jupiter']:\n",
    "    data_for_article['ca'][company]['lat'] = data_for_article['ca'][company].geometry.y\n",
    "    data_for_article['ca'][company]['lon'] = data_for_article['ca'][company].geometry.x\n",
    "    data_for_article['ca'][company] = data_for_article['ca'][company].drop('geometry', axis=1).to_dict()\n",
    "for region in ['nyc', 'nystate']:\n",
    "    for company in ['xdi', 'jupiter']:\n",
    "        for variable in ['Riverine Flooding', 'Coastal Inundation', 'Surface Water Flooding']:\n",
    "            data_for_article[region][company][variable]['lat'] = data_for_article[region][company][variable].geometry.y\n",
    "            data_for_article[region][company][variable]['lon'] = data_for_article[region][company][variable].geometry.x\n",
    "            data_for_article[region][company][variable] = data_for_article[region][company][variable].drop('geometry', axis=1).to_dict()\n",
    "with open(f\"article_data_revised.json\", \"w\") as outfile:\n",
    "    json_object = json.dumps(data_for_article, indent=4)\n",
    "    outfile.write(json_object)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea845f48-e2fe-49d1-9dd2-68ecc45e0f75",
   "metadata": {},
   "outputs": [],
   "source": [
    "consolidated_risks = consolidate_risks(risks).copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "046ba8de-c3d3-4017-a5e5-e41738b026f6",
   "metadata": {},
   "source": [
    "# Calculate percentage above-lowest for each case study"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0f9c2be-548a-4910-8aa7-387467ee4571",
   "metadata": {},
   "source": [
    "## Fire in California"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "474c9fcd-d58f-44a1-9b60-2850612081ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "time_period_naming_dict = {'xdi': {'historical': 'RCP85-1995',\n",
    "                                'future': 'RCP85-2100'},\n",
    "                      'jupiter': {'historical': 'Historical baseline - 1995',\n",
    "                                'future':  'SSP5-8.5 (4.4C) - 2100'}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae126429-a986-4fdf-b7b3-d66c46d27f4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "region = 'California'\n",
    "variable = 'Fire'\n",
    "for period in ['Historical', '2100']:\n",
    "    # remove any locations where there are missing data (NaNs) according to either company\n",
    "    nans_removed = consolidated_risks['ca'][[f'{company} - {variable} - {region} - {period}' \n",
    "                                                        for company in ['Jupiter', 'XDI']]].dropna()\n",
    "    jupiter_percentage = (nans_removed[f'Jupiter - {variable} - {region} - {period}']>1).sum()/len(nans_removed[f'Jupiter - {variable} - {region} - {period}'])\n",
    "    xdi_percentage = (nans_removed[f'XDI - {variable} - {region} - {period}']>1).sum()/len(nans_removed[f'XDI - {variable} - {region} - {period}'])\n",
    "    \n",
    "    print(f\"percentage above lowest for {variable} in {region} {period} jupiter is :{jupiter_percentage:.2g}, xdi is :{xdi_percentage:.2g}, \")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c80aee4a-e531-4dc6-95fe-ebb2b35ca7b0",
   "metadata": {},
   "source": [
    "## Calculating the fraction of locations with increasing fire risk, excluding any which are already the highest risk level in the historical, according to either company."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c69f73c6-85ab-4d1d-86d3-b717649093ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "not_saturated_jupiter = consolidated_risks['ca']['Jupiter - Fire - California - Historical']!=5\n",
    "not_saturated_xdi = consolidated_risks['ca']['XDI - Fire - California - Historical']!=5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdd5ac18-0307-40ff-a9e6-e971aad3184a",
   "metadata": {},
   "outputs": [],
   "source": [
    "not_saturated = consolidated_risks['ca'][not_saturated_jupiter & not_saturated_xdi]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16816456-ea62-46b9-9531-528ec50f8571",
   "metadata": {},
   "outputs": [],
   "source": [
    "for company in ['Jupiter', 'XDI']:\n",
    "    changed = (not_saturated[f'{company} - Fire - California - 2100']>\\\n",
    "               not_saturated[f'{company} - Fire - California - Historical']).sum()/len(not_saturated)\n",
    "    print(f'Fraction of locations increasing according to {company} is {changed}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf50a7e0-d083-4893-afd4-507977a66010",
   "metadata": {},
   "source": [
    "## Coastal inundation in NYC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56ee0f0e-bb67-4cf3-9c04-5752c970461a",
   "metadata": {},
   "outputs": [],
   "source": [
    "region = 'nyc'\n",
    "variable = 'Coastal Inundation'\n",
    "for period in ['Historical', '2100']:\n",
    "            # remove any locations where there are missing data (NaNs) according to either company\n",
    "            nans_removed = consolidated_risks[region][[f'{company} - {variable} - {region} - {period}' \n",
    "                                                                for company in ['Jupiter', 'XDI']]].dropna()\n",
    "            jupiter_percentage = (nans_removed[f'Jupiter - {variable} - {region} - {period}']>1).sum()/len(nans_removed[f'Jupiter - {variable} - {region} - {period}'])\n",
    "            xdi_percentage = (nans_removed[f'XDI - {variable} - {region} - {period}']>1).sum()/len(nans_removed[f'XDI - {variable} - {region} - {period}'])\n",
    "            \n",
    "            print(f\"percentage above lowest for {variable} in {region} {period} jupiter is :{jupiter_percentage:.2g}, xdi is :{xdi_percentage:.2g}, \")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c0e2f44-2e04-4bb3-9ce9-e2fd7d6b450a",
   "metadata": {},
   "source": [
    "## Flooding in NY state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b0965bf-9b4d-4b10-9eec-d305e6da2b06",
   "metadata": {},
   "outputs": [],
   "source": [
    "for region in ['nystate', 'nyc']:\n",
    "    for variable in ['Riverine Flooding', 'Surface Water Flooding']:\n",
    "        for period in ['Historical', '2100']:\n",
    "            # remove any locations where there are missing data (NaNs) according to either company\n",
    "            nans_removed = consolidated_risks[region][[f'{company} - {variable} - {region} - {period}' \n",
    "                                                                for company in ['Jupiter', 'XDI']]].dropna()\n",
    "            jupiter_percentage = (nans_removed[f'Jupiter - {variable} - {region} - {period}']>1).sum()/len(nans_removed[f'Jupiter - {variable} - {region} - {period}'])\n",
    "            xdi_percentage = (nans_removed[f'XDI - {variable} - {region} - {period}']>1).sum()/len(nans_removed[f'XDI - {variable} - {region} - {period}'])\n",
    "            \n",
    "            print(f\"percentage above lowest for {variable} in {region} {period} jupiter is :{jupiter_percentage:.2g}, xdi is :{xdi_percentage:.2g}, \")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09c8db18-7e21-4049-9166-1e6de1e3d46e",
   "metadata": {},
   "source": [
    "# Calculate fraction of locations that increased in risk in the future"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c3f46f3-ac0f-450c-988a-373a59f09b3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "variable='Riverine Flooding'\n",
    "period = 'Historical'\n",
    "region = 'nystate'\n",
    "for region in ['nystate', 'nyc']:\n",
    "    for variable in ['Riverine Flooding', 'Surface Water Flooding', 'Coastal Inundation']:\n",
    "        nans_removed = consolidated_risks[region][[f'{company} - {variable} - {region} - Historical' \n",
    "                                                            for company in ['Jupiter', 'XDI']]+[f'{company} - {variable} - {region} - 2100' \n",
    "                                                            for company in ['Jupiter', 'XDI']]\n",
    "                                                ].dropna()\n",
    "        jupiter_changed = nans_removed[f'Jupiter - {variable} - {region} - 2100']>nans_removed[f'Jupiter - {variable} - {region} - Historical']\n",
    "        jupiter_percentage = jupiter_changed.sum()/len(nans_removed[f'Jupiter - {variable} - {region} - Historical'])\n",
    "        xdi_changed = nans_removed[f'XDI - {variable} - {region} - 2100']>nans_removed[f'XDI - {variable} - {region} - Historical']\n",
    "        xdi_percentage = xdi_changed.sum()/len(nans_removed[f'XDI - {variable} - {region} - Historical'])\n",
    "        \n",
    "        \n",
    "        print(f\"for {variable} in {region} according to jupiter is :{jupiter_percentage:.2g}, and xdi is :{xdi_percentage:.2g}, \")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ecef0f4-faf4-4f1f-8ed8-aed7f0a0a0dd",
   "metadata": {},
   "source": [
    "# Calculate Kendall's Tau test statistic for each case study"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77bea488-0d05-4477-9733-4d72210e7690",
   "metadata": {},
   "source": [
    "## Fire in California"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb159b95-ae55-4b3d-b029-52c27ba34fae",
   "metadata": {},
   "outputs": [],
   "source": [
    "region = 'California'\n",
    "variable = 'Fire'\n",
    "for period in ['Historical', '2100']:\n",
    "    nans_removed = consolidated_risks['ca'][[f'{company} - {variable} - {region} - {period}' \n",
    "                                            for company in ['Jupiter', 'XDI']]].dropna()\n",
    "    tau, p_value, tau_error = calculate_kendalltau(nans_removed[f'Jupiter - {variable} - {region} - {period}'],\n",
    "                     nans_removed[f'XDI - {variable} - {region} - {period}'], variant='b')\n",
    "    print(f\"Tau for {region} {variable} for {period} is {tau:.2g} +- {tau_error:.2g} with p value {p_value:.2g}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d05f10f9-648a-4327-ba57-3f2b93020c59",
   "metadata": {},
   "outputs": [],
   "source": [
    "region = 'California'\n",
    "variable = 'Fire'\n",
    "for period in ['Historical', '2100']:\n",
    "    nans_removed = consolidated_risks['ca'][[f'{company} - {variable} - {region} - {period}' \n",
    "                                            for company in ['Jupiter', 'XDI']]].dropna()\n",
    "    tau, p_value, tau_error = calculate_kendalltau(nans_removed[f'Jupiter - {variable} - {region} - {period}'],\n",
    "                     nans_removed[f'XDI - {variable} - {region} - {period}'], variant='c')\n",
    "    print(f\"Tau for {region} {variable} for {period} is {tau:.2g} +- {tau_error:.2g} with p value {p_value:.2g}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37ed0d1e-fc3e-46f8-98ec-95ae61a3c85b",
   "metadata": {},
   "source": [
    "## Coastal inundation in NYC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d1f53a5-f0e9-4bee-be70-fc110302b3f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "region = 'nyc'\n",
    "variable = 'Coastal Inundation'\n",
    "for period in ['Historical', '2100']:\n",
    "    nans_removed = consolidated_risks[region][[f'{company} - {variable} - {region} - {period}' \n",
    "                                            for company in ['Jupiter', 'XDI']]].dropna()\n",
    "    tau, p_value, tau_error = calculate_kendalltau(nans_removed[f'Jupiter - {variable} - {region} - {period}'],\n",
    "                     nans_removed[f'XDI - {variable} - {region} - {period}'], variant='b')\n",
    "    print(f\"Tau for {region} {variable} for {period} is {tau:.2g} +- {tau_error:.2g} with p value {p_value:.2g}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e1dad25-a2a2-4500-a4f3-eef91e418863",
   "metadata": {},
   "source": [
    "## Flooding in New York state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a105ee1-87b2-4d12-8e37-2d3ffa9809f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "region = 'nystate'\n",
    "for variable in ['Surface Water Flooding', 'Riverine Flooding']:\n",
    "    for period in ['Historical', '2100']:\n",
    "        nans_removed = consolidated_risks[region][[f'{company} - {variable} - {region} - {period}' \n",
    "                                                for company in ['Jupiter', 'XDI']]].dropna()\n",
    "        tau, p_value, tau_error = calculate_kendalltau(nans_removed[f'Jupiter - {variable} - {region} - {period}'],\n",
    "                         nans_removed[f'XDI - {variable} - {region} - {period}'], variant='b')\n",
    "        print(f\"Tau for {region} {variable} for {period} is {tau:.2g} +- {tau_error:.2g} with p value {p_value:.2g}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d90a5ea9-2a53-4085-893c-ff8627f9dd0f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "impacts-impacts-fire",
   "language": "python",
   "name": "conda-env-impacts-impacts-fire-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
